<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <link rel="stylesheet" type="text/css" href="../lib/bootstrap/css/bootstrap.min.css">
    <link rel="stylesheet" type="text/css" href="../lib/font-awesome/css/font-awesome.min.css">
    <link rel="stylesheet" type="text/css" href="../css/style.css">
    <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
    <title>SciMS</title>
</head>
<body>
    <!-- Body of webpage -->
    <section id="scims-article-body" class="container">
        <div class="row">

            <!-- Header of the webpage. -->
            <header class="scims-page-header">
                <h1 class="scims-page-title">
                    Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation
                    <small class="scims-italic scims-header-page-subtitle">Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi yonghui,schuster,zhifengc,qvl,mnorouzi@google.com</small>
                </h1>
            </header>

            <!-- Article Summary -->
            <div class="col-md-3 col-sm-3 col-xs-3">
                <aside id="scims-summary" class="scims-anchor-link">
                    <nav class="navbar">
                        <div class="container-fluid">
                            <!-- Brand and toggle get grouped for better mobile display -->
                            <div id="scims-left-navigation" class="list-group">
                                <a class="list-group-item scims-brand" href="#"> Home</a>
                                <a class="list-group-item scims-connexion" href="#"> Connexion</a>
                            </div>
                        </div>
                    </nav>

                    <!-- Summary of the article : can be undisplayed if you prefer -->
                    <nav>
                        <h3>Summary</h3>
                        <ul class="list-unstyled scims-toggle-visibility">
                            <li><a href="#scims-part-1">1. Introduction</a></li>
                            <li><a href="#scims-part-2">2. Related Work</a></li>
                            <li><a href="#scims-part-3">3. Model Architecture</a>
                                <ul class="list-unstyled scims-sub-level-summary">
                                    <li><a href="#scims-part-3-1">3.1. Residual Connections</a></li>
                                    <li><a href="#scims-part-3-2">3.2.Bi-directional Encoder for First Layer </a></li>
                                    <li><a href="#scims-part-3-3">3.3 Model Parallelism</a></li>
                                </ul>
                            </li>
                            <li><a href="#scims-part-4">4. Segmentation Approaches</a>
                                <ul class="list-unstyled scims-sub-level-summary">
                                    <li><a href="#scims-part-4-1">4.1. Wordpiece Model</a></li>
                                    <li><a href="#scims-part-4-2">4.2. Mixed Word/Character Model</a></li>
                                </ul>
                            </li>
                            <li><a href="#scims-part-5">5. Training Criteria</a></li>
                            <li><a href="#scims-part-6">6. Quantizable Model and Quantized Inference</a></li>
                            <li><a href="#scims-part-7">7. Decoder</a></li>
                            <li><a href="#scims-part-8">8. Experiments and Results</a>
                                <ul class="list-unstyled scims-sub-level-summary">
                                    <li><a href="#scims-part-8-1">8.1. Datasets</a></li>
                                    <li><a href="#scims-part-8-2">8.2. Evaluation Metrics</a></li>
                                    <li><a href="#scims-part-8-3">8.3. Training Procedure</a></li>
                                    <li><a href="#scims-part-8-4">8.4. Evaluation after Maximum Likelihood Training</a></li>
                                    <li><a href="#scims-part-8-5">8.5. Evaluation of RL-refined Models</a></li>
                                    <li><a href="#scims-part-8-6">8.6. Model Ensemble and Human Evaluation</a></li>
                                    <li><a href="#scims-part-8-7">8.7. Results on Production Data</a></li>
                                </ul>
                            </li>
                            <li><a href="#scims-part-9">9. Conclusion</a></li>
                        </ul>
                    </nav>
                </aside>
            </div>

            <!-- Article content -->
            <article class="col-md-offset-1 col-md-8 col-sm-offset-1 col-sm-8 col-xs-offset-1 col-xs-8">
                <h2>Abstract</h2>
                <p>
                    Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation,
                    with the potential to overcome many of the weaknesses of conventional phrase-based translation systems.
                    Unfortunately, NMT systems are known to be computationally expensive both in training and in translation
                    inference – sometimes prohibitively so in the case of very large data sets and large models. Several authors
                    have also charged that NMT systems lack robustness, particularly when input sentences contain rare words.
                    These issues have hindered NMT’s use in practical deployments and services, where both accuracy and
                    speed are essential. In this work, we present GNMT, Google’s Neural Machine Translation system, which
                    attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder
                    and 8 decoder layers using residual connections as well as attention connections from the decoder network
                    to the encoder. To improve parallelism and therefore decrease training time, our attention mechanism
                    connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation
                    speed, we employ low-precision arithmetic during inference computations. To improve handling of rare
                    words, we divide words into a limited set of common sub-word units (“wordpieces”) for both input and
                    output. This method provides a good balance between the flexibility of “character”-delimited models and
                    the efficiency of “word”-delimited models, naturally handles translation of rare words, and ultimately
                    improves the overall accuracy of the system. Our beam search technique employs a length-normalization
                    procedure and uses a coverage penalty, which encourages generation of an output sentence that is most
                    likely to cover all the words in the source sentence. To directly optimize the translation BLEU scores,
                    we consider refining the models by using reinforcement learning, but we found that the improvement
                    in the BLEU scores did not reflect in the human evaluation. On the WMT’14 English-to-French and
                    English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human
                    side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of
                    60% compared to Google’s phrase-based production system.
                </p>
                <section>
                    <h2 id="scims-part-1">1. Introduction</h2>
                    <p>
                        Neural Machine Translation (NMT) [39, 2] has recently been introduced as a promising approach with the
                        potential of addressing many shortcomings of traditional machine translation systems. The strength of NMT
                        lies in its ability to learn directly, in an end-to-end fashion, the mapping from input text to associated
                        output text. Its architecture typically consists of two recurrent neural networks (RNNs), one to consume the
                        input text sequence and one to generate translated output text. NMT is often accompanied by an attention
                        mechanism [2] which helps it cope effectively with long input sequences.
                        An advantage of Neural Machine Translation is that it sidesteps many brittle design choices in traditional
                        phrase-based machine translation [25]. In practice, however, NMT systems used to be worse in accuracy than
                        phrase-based translation systems, especially when training on very large-scale datasets as used for the very
                        best publicly available translation systems. Three inherent weaknesses of Neural Machine Translation are
                        1responsible for this gap: its slower training and inference speed, ineffectiveness in dealing with rare words,
                        and sometimes failure to translate all words in the source sentence. Firstly, it generally takes a considerable
                        amount of time and computational resources to train an NMT system on a large-scale translation dataset,
                        thus slowing the rate of experimental turnaround time and innovation. For inference they are generally
                        much slower than phrase-based systems due to the large number of parameters used. Secondly, NMT lacks
                        robustness in translating rare words. Though this can be addressed in principle by training a “copy model” to
                        mimic a traditional alignment model [30], or by using the attention mechanism to copy rare words [36], these
                        approaches are both unreliable at scale, since the quality of the alignments varies across languages, and the
                        latent alignments produced by the attention mechanism are unstable when the network is deep. Also, simple
                        copying may not always be the best strategy to cope with rare words, for example when a transliteration
                        is more appropriate. Finally, NMT systems sometimes produce output sentences that do not translate all
                        parts of the input sentence – in other words, they fail to completely “cover” the input, which can result in
                        surprising translations.
                        This work presents the design and implementation of GNMT, a production NMT system at Google, that
                        aims to provide solutions to the above problems. In our implementation, the recurrent networks are Long
                        Short-Term Memory (LSTM) RNNs [22, 16]. Our LSTM RNNs have 8 layers, with residual connections
                        between layers to encourage gradient flow [20]. For parallelism, we connect the attention from the bottom
                        layer of the decoder network to the top layer of the encoder network. To improve inference time, we
                        employ low-precision arithmetic for inference, which is further accelerated by special hardware (Google’s
                        Tensor Processing Unit, or TPU). To effectively deal with rare words, we use sub-word units (also known as
                        “wordpieces”) [34] for inputs and outputs in our system. Using wordpieces gives a good balance between the
                        flexibility of single characters and the efficiency of full words for decoding, and also sidesteps the need for
                        special treatment of unknown words. Our beam search technique includes a length normalization procedure
                        to deal efficiently with the problem of comparing hypotheses of different lengths during decoding, and a
                        coverage penalty to encourage the model to translate all of the provided input.
                        Our implementation is robust, and performs well on a range of datasets across many pairs of languages
                        without the need for language-specific adjustments. Using the same implementation, we are able to achieve
                        results comparable to or better than previous state-of-the-art systems on standard benchmarks, while
                        delivering great improvements over Google’s phrase-based production translation system. Specifically, on
                        WMT’14 English-to-French, our single model scores 38.95 BLEU, an improvement of 7.5 BLEU from a
                        single model without an external alignment model reported in [30] and an improvement of 1.2 BLEU from
                        a single model without an external alignment model reported in [43]. Our single model is also comparable
                        to a single model in [43], while not making use of any alignment model as being used in [43]. Likewise on
                        WMT’14 English-to-German, our single model scores 24.17 BLEU, which is 3.4 BLEU better than a previous
                        competitive baseline [6]. On production data, our implementation is even more effective. Human evaluations
                        show that GNMT has reduced translation errors by 60% compared to our previous phrase-based system
                        on many pairs of languages: English ↔ French, English ↔ Spanish, and English ↔ Chinese. Additional
                        experiments suggest the quality of the resulting translation system gets closer to that of average human
                        translators.
                    </p>
                </section>

                <section>
                    <h2 id="scims-part-2">2. Related Work</h2>
                    <p>
                        Statistical Machine Translation (SMT) has been the dominant translation paradigm for decades [3, 4, 5].
                        Practical implementations of SMT are generally phrase-based systems (PBMT) which translate sequences of
                        words or phrases where the lengths may differ [25].
                        Even prior to the advent of direct Neural Machine Translation, neural networks have been used as a
                        component within SMT systems with some success. Perhaps one of the most notable attempts involved the use
                        of a joint language model to learn phrase representations [13] which yielded an impressive improvement when
                        combined with phrase-based translation. This approach, however, still makes use of phrase-based translation
                        systems at its core, and therefore inherits their shortcomings. Other proposed approaches for learning phrase
                        representations [7] or learning end-to-end translation with neural networks [23] offered encouraging hints, but
                        ultimately delivered worse overall accuracy compared to standard phrase-based systems.
                        The concept of end-to-end learning for machine translation has been attempted in the past (e.g., [8]) with
                        2limited success. Following seminal papers in the area [39, 2], NMT translation quality has crept closer to
                        the level of phrase-based translation systems for common research benchmarks. Perhaps the first successful
                        attempt at surpassing phrase-based translation was described in [30]. On WMT’14 English-to-French, this
                        system achieved a 0.5 BLEU improvement compared to a state-of-the-art phrase-based system.
                        Since then, many novel techniques have been proposed to further improve NMT: using an attention
                        mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and
                        semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11],
                        subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and
                        sentence-level loss minimization [38, 33]. While the translation accuracy of these systems has been encouraging,
                        systematic comparison with large scale, production quality phrase-based translation systems has been lacking.
                    </p>
                </section>

                <section>
                    <h2 id="scims-part-3">3. Model Architecture</h2>
                    <p>

                    </p>

                    <h3 id="scims-part-3-1">3.1. Residual Connections</h3>
                    <p>

                    </p>

                    <h3 id="scims-part-3-2">3.2.Bi-directional Encoder for First Layer</h3>
                    <p>

                    </p>

                    <h3 id="scims-part-3-3">3.3 Model Parallelism</h3>
                    <p>

                    </p>
                </section>

                <section>
                    <h2 id="scims-part-4">4. Segmentation Approaches</h2>
                    <p>

                    </p>

                    <h3 id="scims-part-4-1">4.1. Wordpiece Model</h3>
                    <p>

                    </p>

                    <h3 id="scims-part-4-2">4.2. Mixed Word/Character Model</h3>
                    <p>

                    </p>
                </section>

                <section>
                    <h2 id="scims-part-5">5. Training Criteria</h2>
                    <p>

                    </p>
                </section>

                <section>
                    <h2 id="scims-part-6">6. Quantizable Model and Quantized Inference</h2>
                    <p>

                    </p>
                </section>

                <section>
                    <h2 id="scims-part-7">7. Decoder</h2>
                    <p>

                    </p>
                </section>

                <section>
                    <h2 id="scims-part-8">8. Experiments and Results</h2>
                    <p>

                    </p>

                    <h3 id="scims-part-8-1">8.1. Datasets</h3>
                    <p>

                    </p>

                    <h3 id="scims-part-8-2">8.2. Evaluation Metrics</h3>
                    <p>

                    </p>

                    <h3 id="scims-part-8-3">8.3. Training Procedure</h3>
                    <p>

                    </p>

                    <h3 id="scims-part-8-4">8.4. Evaluation after Maximum Likelihood Training</h3>
                    <p>

                    </p>

                    <h3 id="scims-part-8-5">8.5. Evaluation of RL-refined Models</h3>
                    <p>

                    </p>

                    <h3 id="scims-part-8-6">8.6. Model Ensemble and Human Evaluation</h3>
                    <p>

                    </p>

                    <h3 id="scims-part-8-7">8.7. Results on Production Data</h3>
                    <p>

                    </p>
                </section>

                <section>
                    <h2 id="scims-part-9">9. Conclusion</h2>
                    <p>
                        In this paper, we describe in detail the implementation of Google’s Neural Machine Translation (GNMT)
                        system, including all the techniques that are critical to its accuracy, speed, and robustness. On the public
                        WMT’14 translation benchmark, our system’s translation quality approaches or surpasses all currently
                        published results. More importantly, we also show that our approach carries over to much larger production
                        data sets, which have several orders of magnitude more data, to deliver high quality translations.
                        Our key findings are: 1) that wordpiece modeling effectively handles open vocabularies and the challenge
                        of morphologically rich languages for translation quality and inference speed, 2) that a combination of model
                        and data parallelism can be used to efficiently train state-of-the-art sequence-to-sequence NMT models
                        in roughly a week, 3) that model quantization drastically accelerates translation inference, allowing the
                        use of these large models in a deployed production environment, and 4) that many additional details like
                        length-normalization, coverage penalties, and similar are essential to making NMT systems work well on real
                        data.
                        Using human-rated side-by-side comparison as a metric, we show that our GNMT system approaches the
                        19accuracy achieved by average bilingual human translators on some of our test sets. In particular, compared
                        to the previous phrase-based production system, this GNMT system delivers roughly a 60% reduction in
                        translation errors on several popular language pairs.
                    </p>

                    <h3>Acknowledgements</h3>
                    <p>
                        We would like to thank the entire Google Brain Team and Google Translate Team for their foundational
                        contributions to this project.
                    </p>

                    <h3>References</h3>
                    <ol>
                        <li>Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S.,
                            Irving, G., Isard, M., Kudlur, M., Levenberg, J., Monga, R., Moore, S., Murray, D. G.,
                            Steiner, B., Tucker, P., Vasudevan, V., Warden, P., Wicke, M., Yu, Y., and Zheng, X.
                            Tensorflow: A system for large-scale machine learning. Tech. rep., Google Brain, 2016. arXiv preprint.</li>
                        <li>Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align
                            and translate. In International Conference on Learning Representations (2015).</li>
                        <li>Brown, P., Cocke, J., Pietra, S. D., Pietra, V. D., Jelinek, F., Mercer, R., and Roossin, P.
                            A statistical approach to language translation. In Proceedings of the 12th Conference on Computational
                            Linguistics - Volume 1 (Stroudsburg, PA, USA, 1988), COLING ’88, Association for Computational
                            Linguistics, pp. 71–76.</li>
                        <li>Brown, P. F., Cocke, J., Pietra, S. A. D., Pietra, V. J. D., Jelinek, F., Lafferty, J. D.,
                            Mercer, R. L., and Roossin, P. S. A statistical approach to machine translation. Computational
                            linguistics 16, 2 (1990), 79–85.</li>
                        <li>Brown, P. F., Pietra, V. J. D., Pietra, S. A. D., and Mercer, R. L. The mathematics of
                            statistical machine translation: Parameter estimation. Comput. Linguist. 19, 2 (June 1993), 263–311.</li>
                        <li>Buck, C., Heafield, K., and Van Ooyen, B. N-gram counts and language models from the common
                            crawl. In LREC (2014), vol. 2, Citeseer, p. 4.</li>
                        <li>Cho, K., van Merrienboer, B., Gülçehre, Ç., Bougares, F., Schwenk, H., and Bengio,
                            Y. Learning phrase representations using RNN encoder-decoder for statistical machine translation. In
                            Conference on Empirical Methods in Natural Language Processing (2014).</li>
                        <li>Chrisman, L. Learning recursive distributed representations for holistic computation. Connection
                            Science 3, 4 (1991), 345–366.</li>
                        <li>Chung, J., Cho, K., and Bengio, Y. A character-level decoder without explicit segmentation for
                            neural machine translation. arXiv preprint arXiv:1603.06147 (2016).</li>
                        <li>Chung, J., Cho, K., and Bengio, Y. A character-level decoder without explicit segmentation for
                            neural machine translation. CoRR abs/1603.06147 (2016).</li>
                        <li>Costa-Jussà, M. R., and Fonollosa, J. A. R. Character-based neural machine translation. CoRR
                            abs/1603.00810 (2016).</li>
                        <li>Dean, J., Corrado, G. S., Monga, R., Chen, K., Devin, M., Le, Q. V., Mao, M. Z., Ranzato,
                            M., Senior, A., Tucker, P., Yang, K., and Ng, A. Y. Large scale distributed deep networks. In
                            NIPS (2012).</li>
                        <li>Devlin, J., Zbib, R., Huang, Z., Lamar, T., Schwartz, R. M., and Makhoul, J. Fast and
                            robust neural network joint models for statistical machine translation. In ACL (1) (2014), Citeseer,
                            pp. 1370–1380.</li>
                        <li>Dong, D., Wu, H., He, W., Yu, D., and Wang, H. Multi-task learning for multiple language
                            translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
                            (2015), pp. 1723–1732.</li>
                        <li>Durrani, N., Haddow, B., Koehn, P., and Heafield, K. Edinburgh’s phrase-based machine
                            translation systems for WMT-14. In Proceedings of the Ninth Workshop on Statistical Machine Translation
                            (2014), Association for Computational Linguistics Baltimore, MD, USA, pp. 97–104.</li>
                        <li>Gers, F. A., Schmidhuber, J., and Cummins, F. Learning to forget: Continual prediction with
                            LSTM. Neural computation 12, 10 (2000), 2451–2471.</li>
                        <li>Gülçehre, Ç., Ahn, S., Nallapati, R., Zhou, B., and Bengio, Y. Pointing the unknown words.
                            CoRR abs/1603.08148 (2016).</li>
                        <li>Gupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan, P. Deep learning with limited
                            numerical precision. CoRR abs/1502.02551 (2015).</li>
                        <li>Han, S., Mao, H., and Dally, W. J. Deep compression: Compressing deep neural network with
                            pruning, trained quantization and huffman coding. CoRR abs/1510.00149 (2015).</li>
                        <li>He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In IEEE
                            Conference on Computer Vision and Pattern Recognition (2015).</li>
                        <li>Hochreiter, S., Bengio, Y., Frasconi, P., and Schmidhuber, J. Gradient flow in recurrent nets:
                            the difficulty of learning long-term dependencies, 2001.</li>
                        <li>Hochreiter, S., and Schmidhuber, J. Long short-term memory. Neural computation 9, 8 (1997),
                            1735–1780.</li>
                        <li>Kalchbrenner, N., and Blunsom, P. Recurrent continuous translation models. In Conference on
                            Empirical Methods in Natural Language Processing (2013).</li>
                        <li>Kingma, D. P., and Ba, J. Adam: A method for stochastic optimization. CoRR abs/1412.6980
                            (2014).</li>
                        <li>Koehn, P., Och, F. J., and Marcu, D. Statistical phrase-based translation. In Proceedings of the
                            2003 Conference of the North American Chapter of the Association for Computational Linguistics (2003).</li>
                        <li>Li, F., and Liu, B. Ternary weight networks. CoRR abs/1605.04711 (2016).</li>
                        <li>Luong, M., and Manning, C. D. Achieving open vocabulary neural machine translation with hybrid
                            word-character models. CoRR abs/1604.00788 (2016).</li>
                        <li>Luong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., and Kaiser, L. Multi-task sequence to
                            sequence learning. In International Conference on Learning Representations (2015).</li>
                        <li>Luong, M.-T., Pham, H., and Manning, C. D. Effective approaches to attention-based neural
                            machine translation. In Conference on Empirical Methods in Natural Language Processing (2015).</li>
                        <li>Luong, M.-T., Sutskever, I., Le, Q. V., Vinyals, O., and Zaremba, W. Addressing the rare word
                            problem in neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association for
                            Computational Linguistics and the 7th International Joint Conference on Natural Language Processing
                            (2015).</li>
                        <li>Norouzi, M., Bengio, S., Chen, Z., Jaitly, N., Schuster, M., Wu, Y., and Schuurmans,
                            D. Reward augmented maximum likelihood for neural structured prediction. In Neural Information
                            Processing Systems (2016).</li>
                        <li>Pascanu, R., Mikolov, T., and Bengio, Y. Understanding the exploding gradient problem. CoRR
                            abs/1211.5063 (2012).</li>
                        <li>Ranzato, M., Chopra, S., Auli, M., and Zaremba, W. Sequence level training with recurrent
                            neural networks. In International Conference on Learning Representations (2015).</li>
                        <li>Schuster, M., and Nakajima, K. Japanese and Korean voice search. 2012 IEEE International
                            Conference on Acoustics, Speech and Signal Processing (2012).</li>
                        <li>Schuster, M., and Paliwal, K. Bidirectional recurrent neural networks. IEEE Transactions on
                            Signal Processing 45, 11 (Nov. 1997), 2673–2681.</li>
                        <li>Sébastien, J., Kyunghyun, C., Memisevic, R., and Bengio, Y. On using very large target
                            vocabulary for neural machine translation. In Proceedings of the 53rd Annual Meeting of the Association
                            for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing
                            (2015).</li>
                        <li>Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword
                            units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (2016).</li>
                        <li>Shen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M., and Liu, Y. Minimum risk training
                            for neural machine translation. In Proceedings of the 54th Annual Meeting of the Association for
                            Computational Linguistics (2016).</li>
                        <li>Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. In
                            Advances in Neural Information Processing Systems (2014), pp. 3104–3112.</li>
                        <li>Tu, Z., Lu, Z., Liu, Y., Liu, X., and Li, H. Coverage-based neural machine translation. In Proceedings
                            of the 54th Annual Meeting of the Association for Computational Linguistics (2016).</li>
                        <li>Wu, J., Leng, C., Wang, Y., Hu, Q., and Cheng, J. Quantized convolutional neural networks for
                            mobile devices. CoRR abs/1512.06473 (2015).</li>
                        <li>Zaremba, W., Sutskever, I., and Vinyals, O. Recurrent neural network regularization, 2014.</li>
                        <li>Zhou, J., Cao, Y., Wang, X., Li, P., and Xu, W. Deep recurrent models with fast-forward
                            connections for neural machine translation. CoRR abs/1606.04199 (2016).</li>
                    </ol>
                </section>
            </article>

            <!-- Footer content -->
            <footer>
                <p></p>
            </footer>
        </div>

        <!-- Loading script section -->
        <script src="../lib/jquery/jquery-3.1.1.min.js"></script>
        <script src="../lib/bootstrap/js/bootstrap.min.js"></script>
    </section>

    <!-- Footer of the webpage. -->
    <footer>
        <p class="scims-italic scims-header-page-subtitle">Nicolas GILLE, Grégoire POMMIER, &copy; 2016, SciMS : la rédaction de vos articles de recherches simplifiées.</p>
    </footer>
</body>
</html>